import json
import numpy as np
import argparse
from tqdm import tqdm
import torch
from sklearn.metrics import precision_score, recall_score, f1_score, ndcg_score
import json
import os
import torch

from sentence_transformers import SentenceTransformer
from transformers import BitsAndBytesConfig

# setting gRPC to avoid warnings and timeouts
os.environ["GRPC_KEEPALIVE_TIME_MS"] = "300000"
os.environ["GRPC_KEEPALIVE_TIMEOUT_MS"] = "20000"
os.environ["GRPC_KEEPALIVE_PERMIT_WITHOUT_CALLS"] = "0"
os.environ["GRPC_HTTP2_MIN_TIME_BETWEEN_PINGS_MS"] = "120000"
os.environ["GRPC_HTTP2_MIN_RECV_PING_INTERVAL_WITHOUT_DATA_MS"] = "300000"
from pymilvus import MilvusClient, CollectionSchema, FieldSchema, DataType
from pymilvus import AnnSearchRequest, Function, FunctionType
from pymilvus import RRFRanker, WeightedRanker

import os

def ingest_collection(client, data, collection_name = "text_collection", emb_dim=768, remove_if_exists=False):

    if client.has_collection(collection_name=collection_name):
        if remove_if_exists:
            client.drop_collection(collection_name=collection_name)
        else:
            print(f"Collection {collection_name} already exists â€” skipping creation and insert.")
            return

    schema = MilvusClient.create_schema(auto_id=False)

    schema.add_field(field_name="id", datatype=DataType.INT64, is_primary=True, auto_id=True)
    schema.add_field(field_name="uuid",datatype=DataType.VARCHAR,max_length=64)
    schema.add_field(field_name="content", datatype=DataType.VARCHAR, max_length=65534,enable_analyzer=True, description="raw content")
    schema.add_field(field_name="content_dense", datatype=DataType.FLOAT_VECTOR, dim=emb_dim, description="content dense embedding")
    schema.add_field(field_name="content_sparse", datatype=DataType.SPARSE_FLOAT_VECTOR, description="content sparse embedding auto-generated by the built-in BM25 function")

    bm25_function = Function(
        name="text_bm25_emb",
        input_field_names=["content"],
        output_field_names=["content_sparse"],
        function_type=FunctionType.BM25,
    )
    schema.add_function(bm25_function)

    # create index
    index_params = client.prepare_index_params()

    index_params.add_index(
        field_name="content_dense",
        index_name="content_dense_index",
        index_type="AUTOINDEX",
        metric_type="IP"
    )

    index_params.add_index(
        field_name="content_sparse",
        index_name="content_sparse_index",
        index_type="SPARSE_INVERTED_INDEX",
        metric_type="BM25",
        params={"inverted_index_algo": "DAAT_MAXSCORE"}, # or "DAAT_WAND" or "TAAT_NAIVE"
    )

    # create collection
    client.create_collection(
        collection_name=collection_name,
        schema=schema,
        index_params=index_params
    )

    # insert data
    print(f" inserting: {collection_name} {len(data)}")
    chunk_size = 25000 # inserting in chunks of 10,000 avoid issues with gRPC
    for i in range(0, len(data), chunk_size):
        print(f"    chunk from {i} to {i+chunk_size}")
        chunk = data[i: i+chunk_size]
        res = client.insert(
            collection_name=collection_name,
            data=chunk
        )
    print(f"Inserted {res.get('insert_count', len(data))} rows")

    client.flush(collection_name)
    client.load_collection(collection_name)

def hybrid_search(client, query_content, query_emb, collection_name = "text_collection", top_k = 2, rerank_config= "rrf"):
    query_text = query_content
    query_dense_vector = query_emb

    search_param_1 = {
        "data": [query_dense_vector],
        "anns_field": "content_dense",
        "param": {"nprobe": 10, "metric_type": "IP"},
        "limit": top_k
    }
    request_1 = AnnSearchRequest(**search_param_1)

    search_param_2 = {
        "data": [query_text],
        "anns_field": "content_sparse",
        "param": {"drop_ratio_search": 0.2},
        "limit": top_k
    }
    request_2 = AnnSearchRequest(**search_param_2)

    reqs = [request_1, request_2]

    if rerank_config == "rrf":
        ranker = RRFRanker(60)
    elif rerank_config == "vector_only":
        ranker = WeightedRanker(1, 0)
    elif rerank_config == "bm25_only":
        ranker = WeightedRanker(0, 1)
    else:
        raise ValueError(f"Unknown reranker config: {rerank_config}")

    res = client.hybrid_search(
        collection_name=collection_name,
        reqs=reqs,
        ranker=ranker,
        limit=top_k*2,
        output_fields=["uuid"]
    )

    ranked = []
    for hits in res:
        scores = {}
        for hit in hits:
            uid = hit.entity.get("uuid")
            scores[uid] = hit.score
        ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        ranked = [x for x in ranked if x[1] > 0]
    return ranked


def save_jsonl(results, path):
    with open(path, "w", encoding="utf-8") as f:
        for r in results:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

def get_table_representation(table_data, config=1):
    #TODO we need to explore different options for this representation 
    title = table_data["title"]
    headers  = table_data["header"]
    tbl_rep = title + " | ".join(headers)
    return tbl_rep

def prepare_data(corpus_path, embedding_model, device):

    # corpus
    with open(corpus_path, "r", encoding="utf-8") as f:
        corpus = json.load(f)

    table_uuids, table_content, text_uuids, text_content  = [], [], [], []
    # using a set for look up to avoid duplicates
    seen_text_uuids = set()
    for entry in tqdm(corpus, desc="Preparing embeddings"):
        uid  = entry["table"]["uid"]
        if uid in table_uuids:
            continue

        # serialize a table to a text representation
        table_rep = get_table_representation(entry["table"])
        table_content.append(table_rep)
        table_uuids.append(uid)

        # a single table can have multiple text passages associated
        for text_passage in entry["synth_text"]:
            # we only add the text one time
            if text_passage['uid'] in seen_text_uuids:
                continue
            else:
                text_uuids.append(text_passage['uid'])
                text_content.append(f"Tile: {text_passage['title']}\nContent: {text_passage['text']}")
                seen_text_uuids.add(text_passage['uid'])

    print(f"loaded {len(table_uuids)} tables and {len(text_uuids)} text passages.")
        
    # prepare data 
    run_parallel = True

    if torch.cuda.is_available() and run_parallel:
        use_bf16 = torch.cuda.get_device_capability()[0] >= 8
        autocast_dtype = torch.bfloat16 if use_bf16 else torch.float16

        num_workers = 8
        print("Computing table embeddings...")
        with torch.inference_mode(), torch.autocast("cuda", dtype=autocast_dtype):
            table_embeddings = embedding_model.encode(
                table_content,
                batch_size=512, # 24, 512
                convert_to_tensor=True,      
                normalize_embeddings=True, 
                show_progress_bar=True,
                device=device,
                num_workers=num_workers,   
                pin_memory=True
            )

        print("Computing text embeddings...")
        with torch.inference_mode(), torch.autocast("cuda", dtype=autocast_dtype):
            text_embeddings = embedding_model.encode(
                text_content,
                batch_size=512, # 24, 512
                convert_to_tensor=True,      
                normalize_embeddings=True, 
                show_progress_bar=True,
                device=device,
                num_workers=num_workers,   
                pin_memory=True
            )
    else:
        print("Run parallel: False")

        print("Computing table embeddings...")
        table_embeddings = embedding_model.encode(table_content,
                                                  batch_size=1, 
                                                  normalize_embeddings=True, 
                                                  show_progress_bar=True, 
                                                  device=device,     
                                                  model_kwargs={
        "torch_dtype":"bfloat16",     # or "float16"
        "device_map":"auto",
        "max_memory": {0: "60GiB", "cpu": "64GiB"},
        "quantization_config": BitsAndBytesConfig(load_in_8bit=True)
    })

        print("Computing text embeddings...")
        text_embeddings = embedding_model.encode(text_content, 
                                                 batch_size=1, 
                                                 normalize_embeddings=True, 
                                                 show_progress_bar=True, 
                                                 device=device,     
                                                 model_kwargs={
        "torch_dtype":"bfloat16",     # or "float16"
        "device_map":"auto",
        "max_memory": {0: "60GiB", "cpu": "64GiB"},
        "quantization_config": BitsAndBytesConfig(load_in_8bit=True)
    })

    table_rows = [
        {"uuid": uid, "content": table_info, "content_dense": tblvec}
        for uid, table_info, tblvec in zip(table_uuids, table_content, table_embeddings.tolist())
    ]

    text_rows = [
         {"uuid": uid, "content": text_info, "content_dense": text_vec}
        for uid, text_info, text_vec in zip(text_uuids, text_content, text_embeddings.tolist())       
    ]

    return table_rows, text_rows


def prepare_queries(dpr_path, embedding_model, device):

    with open(dpr_path, "r", encoding="utf-8") as f:
        dprs = [json.loads(line) for line in f if line.strip()]
    print(f"{len(dprs)} queries loaded.")

    ids = [dpr["dpr_id"] for dpr in dprs]
    queries = [dpr["DPR"] for dpr in dprs]
    query_emeddings = embedding_model.encode(queries, normalize_embeddings=True, show_progress_bar=True, device=device)
    query_rows = [
        {"dpr_id": dpr_id, "query": query, "embedding": query_embedding} for dpr_id, query, query_embedding in zip(ids, queries, query_emeddings)
    ]
    return query_rows

def main(args):
    db_name = args.db
    data_name = db_name[:-3]
    corpus_path = args.corpus
    model_name = args.model
    dpr_path = args.dpr
    output_path = args.output_path
    top_k = 100
    force_clean = False
    emb_dim= args.emb_dim

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"using device: {device}, emb dim: {emb_dim}")

    if torch.cuda.is_available():
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.set_float32_matmul_precision("high")

    embedding_model = SentenceTransformer(model_name, device=device)
    embedding_model.eval()

    # just delete the file if it exists to make sure a clean run
    if force_clean and os.path.exists(db_name):
        os.remove(db_name)

    client = MilvusClient(db_name)
    if client.has_collection(collection_name="text_collection") and client.has_collection(collection_name="table_collection"):
        print("both collections exist. skipping the ingestion")
    else:
        print("collections does not exist. Ingesting data ...")
        table_rows, text_rows = prepare_data(corpus_path, embedding_model, device)
        ingest_collection(client, text_rows, "text_collection", emb_dim)
        ingest_collection(client, table_rows, "table_collection", emb_dim)

    query_results = []
    print("Computing the query embeddings ...")
    query_rows = prepare_queries(dpr_path, embedding_model, device)
    total_rrf, total_overlap_text, total_overlap_table = 0,0,0
    for query_row in tqdm(query_rows, total=len(query_rows), desc="Generating query results"):
        # print("Query:", query_row['query'])
        query_result = {"dpr_id": query_row['dpr_id'], "dpr": query_row['query'], "results": {}}
        for rerank_config in ['rrf', 'bm25_only', 'vector_only']:
            text_rank_list = hybrid_search(client, query_row['query'], query_row['embedding'], collection_name = "text_collection", top_k = top_k, rerank_config=rerank_config)
            table_rank_list = hybrid_search(client, query_row['query'], query_row['embedding'], collection_name = "table_collection", top_k = top_k, rerank_config=rerank_config)
            txt_tbl_results = {"text": text_rank_list, "table": table_rank_list}
            query_result["results"][rerank_config] = txt_tbl_results
            if rerank_config == 'rrf':
                total_rrf += 1
                total_overlap_table += top_k*2-len(table_rank_list)
                total_overlap_text += top_k*2-len(text_rank_list)
                # print(f" rerank config: {rerank_config}")
                # print(f"    text result count {len(text_rank_list)} (overlap: {top_k*2-len(text_rank_list)})")
                # print(f"    table result count {len(table_rank_list)} (overlap: {top_k*2-len(table_rank_list)})\n")
        query_results.append(query_result)

    print(f"Average overlap:\n\tText: {total_overlap_text/total_rrf:.4f}\n\tTable: {total_overlap_table/total_rrf:.4f}")

    result_summary = {
        "dataset": data_name,
        "embedding_model": model_name,
        "results": query_results
    }

    with open(output_path, "w") as out_file:
        json.dump(result_summary, out_file, ensure_ascii=False, indent=1)

if __name__ == "__main__":
    p = argparse.ArgumentParser(description="DPR benchmark with Milvus")
    p.add_argument("--corpus",      type=str, default="data/output/corpus.json")
    p.add_argument("--dpr",         type=str, default="data/output/dprs_final.json")
    p.add_argument("--db",          type=str, default="TATQA.db")
    p.add_argument("--dataset",          type=str, default="TATQA")
    p.add_argument("--collection",  type=str, default="dpr_benchmark")
    p.add_argument("--model",  type=str, default="sentence-transformers/all-mpnet-base-v2")
    p.add_argument("--output_path",         type=str, default="data/output/results.json")
    p.add_argument("--force_clean",         type=bool, default=False)
    
    p.add_argument("--emb_dim",     type=int, default=768)
    p.add_argument("--index-type",  type=str, default="AUTOINDEX")
    p.add_argument("--metric-type", type=str, default="IP")
    p.add_argument("--top_text",    type=int, default=20)
    p.add_argument("--top_table",   type=int, default=20)

    args = p.parse_args()
    main(args)
